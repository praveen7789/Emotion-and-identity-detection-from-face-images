{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-1 importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import dlib\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import facevec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from imutils import face_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(input_image, gamma=1.0):\n",
    "    table = np.array([((iteration / 255.0) ** (1.0 / gamma)) * 255\n",
    "                      for iteration in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(input_image, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, gamma=0.75):\n",
    "    output = cv2.imread(path)\n",
    "    return adjust_gamma(output, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_vector(input_image):\n",
    "    faces = facevec.detector(input_image, 1)\n",
    "    if not faces:\n",
    "        return None\n",
    "\n",
    "    f = faces[0]\n",
    "    shape = facevec.predictor(input_image, f)\n",
    "    face_descriptor = facevec.face_model.compute_face_descriptor(input_image, shape)\n",
    "    return face_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = 0\n",
    "female = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Male images ...\n",
      "Retrieved 75 faces !\n",
      "Retrieving female images ...\n",
      "Retrieved 64 faces !\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving Male images ...\")\n",
    "sub1 = glob.glob(\"./data/face_data/male/*.png\")\n",
    "print(\"Retrieved {} faces !\".format(len(sub1)))\n",
    "\n",
    "print(\"Retrieving female images ...\")\n",
    "sub2 = glob.glob(\"./data/face_data/female/*.png\")\n",
    "print(\"Retrieved {} faces !\".format(len(sub2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = dlib.vectors()\n",
    "labels = dlib.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Males images ...\n",
      "Reading 0 of 75\n",
      "Reading 1 of 75\n",
      "Reading 2 of 75\n",
      "Reading 3 of 75\n",
      "Reading 4 of 75\n",
      "Reading 5 of 75\n",
      "Reading 6 of 75\n",
      "Reading 7 of 75\n",
      "Reading 8 of 75\n",
      "Reading 9 of 75\n",
      "Reading 10 of 75\n",
      "Reading 11 of 75\n",
      "Reading 12 of 75\n",
      "Reading 13 of 75\n",
      "Reading 14 of 75\n",
      "Reading 15 of 75\n",
      "Reading 16 of 75\n",
      "Reading 17 of 75\n",
      "Reading 18 of 75\n",
      "Reading 19 of 75\n",
      "Reading 20 of 75\n",
      "Reading 21 of 75\n",
      "Reading 22 of 75\n",
      "Reading 23 of 75\n",
      "Reading 24 of 75\n",
      "Reading 25 of 75\n",
      "Reading 26 of 75\n",
      "Reading 27 of 75\n",
      "Reading 28 of 75\n",
      "Reading 29 of 75\n",
      "Reading 30 of 75\n",
      "Reading 31 of 75\n",
      "Reading 32 of 75\n",
      "Reading 33 of 75\n",
      "Reading 34 of 75\n",
      "Reading 35 of 75\n",
      "Reading 36 of 75\n",
      "Reading 37 of 75\n",
      "Reading 38 of 75\n",
      "Reading 39 of 75\n",
      "Reading 40 of 75\n",
      "Reading 41 of 75\n",
      "Reading 42 of 75\n",
      "Reading 43 of 75\n",
      "Reading 44 of 75\n",
      "Reading 45 of 75\n",
      "Reading 46 of 75\n",
      "Reading 47 of 75\n",
      "Reading 48 of 75\n",
      "Reading 49 of 75\n",
      "Reading 50 of 75\n",
      "Reading 51 of 75\n",
      "Reading 52 of 75\n",
      "Reading 53 of 75\n",
      "Reading 54 of 75\n",
      "Reading 55 of 75\n",
      "Reading 56 of 75\n",
      "Reading 57 of 75\n",
      "Reading 58 of 75\n",
      "Reading 59 of 75\n",
      "Reading 60 of 75\n",
      "Reading 61 of 75\n",
      "Reading 62 of 75\n",
      "Reading 63 of 75\n",
      "Reading 64 of 75\n",
      "Reading 65 of 75\n",
      "Reading 66 of 75\n",
      "Reading 67 of 75\n",
      "Reading 68 of 75\n",
      "Reading 69 of 75\n",
      "Reading 70 of 75\n",
      "Reading 71 of 75\n",
      "Reading 72 of 75\n",
      "Reading 73 of 75\n",
      "Reading 74 of 75\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Males images ...\")\n",
    "for i, sub in enumerate(sub1):\n",
    "    print(\"Reading {} of {}\\r\".format(i, len(sub1)))\n",
    "    face_vectors = face_vector(read_image(sub))\n",
    "    if face_vectors is None:\n",
    "        continue\n",
    "    vectors.append(dlib.vector(face_vectors))\n",
    "    labels.append(male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Female  images ...\n",
      "Reading 0 of 64\n",
      "Reading 1 of 64\n",
      "Reading 2 of 64\n",
      "Reading 3 of 64\n",
      "Reading 4 of 64\n",
      "Reading 5 of 64\n",
      "Reading 6 of 64\n",
      "Reading 7 of 64\n",
      "Reading 8 of 64\n",
      "Reading 9 of 64\n",
      "Reading 10 of 64\n",
      "Reading 11 of 64\n",
      "Reading 12 of 64\n",
      "Reading 13 of 64\n",
      "Reading 14 of 64\n",
      "Reading 15 of 64\n",
      "Reading 16 of 64\n",
      "Reading 17 of 64\n",
      "Reading 18 of 64\n",
      "Reading 19 of 64\n",
      "Reading 20 of 64\n",
      "Reading 21 of 64\n",
      "Reading 22 of 64\n",
      "Reading 23 of 64\n",
      "Reading 24 of 64\n",
      "Reading 25 of 64\n",
      "Reading 26 of 64\n",
      "Reading 27 of 64\n",
      "Reading 28 of 64\n",
      "Reading 29 of 64\n",
      "Reading 30 of 64\n",
      "Reading 31 of 64\n",
      "Reading 32 of 64\n",
      "Reading 33 of 64\n",
      "Reading 34 of 64\n",
      "Reading 35 of 64\n",
      "Reading 36 of 64\n",
      "Reading 37 of 64\n",
      "Reading 38 of 64\n",
      "Reading 39 of 64\n",
      "Reading 40 of 64\n",
      "Reading 41 of 64\n",
      "Reading 42 of 64\n",
      "Reading 43 of 64\n",
      "Reading 44 of 64\n",
      "Reading 45 of 64\n",
      "Reading 46 of 64\n",
      "Reading 47 of 64\n",
      "Reading 48 of 64\n",
      "Reading 49 of 64\n",
      "Reading 50 of 64\n",
      "Reading 51 of 64\n",
      "Reading 52 of 64\n",
      "Reading 53 of 64\n",
      "Reading 54 of 64\n",
      "Reading 55 of 64\n",
      "Reading 56 of 64\n",
      "Reading 57 of 64\n",
      "Reading 58 of 64\n",
      "Reading 59 of 64\n",
      "Reading 60 of 64\n",
      "Reading 61 of 64\n",
      "Reading 62 of 64\n",
      "Reading 63 of 64\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Female  images ...\")\n",
    "for i, sub in enumerate(sub2):\n",
    "    print(\"Reading {} of {}\\r\".format(i, len(sub2)))\n",
    "    face_vectors = face_vector(read_image(sub))\n",
    "    if face_vectors is None:\n",
    "        continue\n",
    "    vectors.append(dlib.vector(face_vectors))\n",
    "    labels.append(female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dlib.vectors"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Saving into csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec\n",
    "y = lab.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.append(X,y,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('face_vectors.csv',dataset,delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn.fit(x_train, y_train) # training model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = model_knn.predict(x_test) # we use this for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_knn = confusion_matrix(y_test, y_pred_knn) # confusion matrix\n",
    "\n",
    "\n",
    "cr_knn = classification_report(y_test, y_pred_knn) # classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm_knn,annot=True,cbar=None,cmap = 'summer')\n",
    "plt.title('K Nearest Neighbour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================KNearest Neighbour====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94        18\n",
      "         1.0       0.82      1.00      0.90         9\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        27\n",
      "   macro avg       0.91      0.94      0.92        27\n",
      "weighted avg       0.94      0.93      0.93        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('='*20+'KNearest Neighbour'+'='*20)\n",
    "print(cr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender_class.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_knn,'gender_class.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desison Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_DT = DT_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_DT = confusion_matrix(y_test, y_pred_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_DT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
